{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afa03b1e-fa92-400e-a54a-3739d8d34c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39cd57ab-df2c-44f9-bd4d-10067f3a30b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(64000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e337331-6391-4c4b-81e2-2629f322466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure `idx` is within the bounds of `self.encodings` and `self.labels`\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9e6e59c-27ad-44b8-8621-32d593784c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    try:\n",
    "        # Attempt to read the file assuming it's line-delimited or an array of objects\n",
    "        data = pd.read_json(file_path, lines=True)\n",
    "    except ValueError:\n",
    "        try:\n",
    "            # For a single JSON object, read it into a Series and then convert to a DataFrame\n",
    "            data = pd.read_json(file_path, typ='series')\n",
    "            data = pd.DataFrame([data])  # Convert Series to DataFrame\n",
    "        except ValueError as e:\n",
    "            print(f\"Failed to read {file_path}: {e}\")\n",
    "            data = pd.DataFrame()  # Return an empty DataFrame on failure\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89f060ef-f4f4-46b3-99d1-4aa396a24dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_files(directory):\n",
    "    files = glob.glob(f\"{directory}/*.json\")\n",
    "    data_frames = [read_json_file(file) for file in files]\n",
    "    # Filter out empty DataFrames\n",
    "    data_frames = [df for df in data_frames if not df.empty]\n",
    "    if data_frames:\n",
    "        # Concatenate all DataFrames\n",
    "        return pd.concat(data_frames, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No valid JSON data could be loaded.\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "197f3ec7-d91c-4d97-8100-0a41de534b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5977.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5958.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5931.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6048.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5830.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5750.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/594.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6651.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6512.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6087.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5928.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6313.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6126.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6170.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6060.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6514.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6643.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6284.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5869.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6205.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6491.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6018.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6646.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6273.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6275.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6219.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6456.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6069.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/621.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6551.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6640.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5925.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6124.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6420.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5930.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6369.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5807.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/653.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5803.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6274.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6615.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6441.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6490.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6339.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6334.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6346.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6321.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6480.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6117.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/627.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5818.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6063.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6631.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6431.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6499.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5915.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6078.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5961.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6620.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6057.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6066.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/638.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6020.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5765.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6240.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5966.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5757.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5785.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5912.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5963.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6201.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6204.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/585.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6531.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5929.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/663.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6458.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6222.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6419.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5888.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5866.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6614.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5903.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5771.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5820.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/63.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6318.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6525.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6495.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/589.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6177.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6115.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6479.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6422.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5964.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6417.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6145.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6047.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6370.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6611.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6037.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5745.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6603.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6629.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6377.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6265.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6616.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6252.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/65.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6588.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6121.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6154.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6452.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/616.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6493.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6653.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6039.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6521.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6001.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6269.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6552.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/598.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6569.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5808.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5848.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5811.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6541.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6283.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6598.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6187.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/637.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6019.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5843.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/642.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5942.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/587.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5758.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5997.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5814.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5878.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6025.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5749.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6036.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5747.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5919.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/5844.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/657.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6262.json: Expected object or value\n",
      "Failed to read /home/yousef-tarek-st/LiteraryHub-ML/src/nlp/restricted_topic_detection/labeled_dataset/6582.json: Expected object or value\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_name</th>\n",
       "      <th>page_number</th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>خماسية مدن الملح</td>\n",
       "      <td>811</td>\n",
       "      <td>يا سيدي الله يخليك المسألة من أولها إلى آخرها...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>الفرق و المذاهب في الرسالات الثلاث</td>\n",
       "      <td>56</td>\n",
       "      <td>لتعاليم بولسء ويعدونها من الهرطقة واندثرت هذه ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>الحارس في حقل الشوفان</td>\n",
       "      <td>39</td>\n",
       "      <td>أمي الحبيية ماذا لا قدين لي يدكدة كنت أعيث فقط...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>خماسية مدن الملح</td>\n",
       "      <td>902</td>\n",
       "      <td>أي نعم كيف لا أتذكر  صارت خمسة وعشرين ألف دول...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>خماسية مدن الملح</td>\n",
       "      <td>143</td>\n",
       "      <td>بدت الحدرة وهما يغادرانها أكثر حزبا وأكثر شيخو...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            book_name  page_number  \\\n",
       "0                    خماسية مدن الملح          811   \n",
       "1  الفرق و المذاهب في الرسالات الثلاث           56   \n",
       "2               الحارس في حقل الشوفان           39   \n",
       "3                    خماسية مدن الملح          902   \n",
       "4                    خماسية مدن الملح          143   \n",
       "\n",
       "                                          text_chunk  label  confidence  \n",
       "0   يا سيدي الله يخليك المسألة من أولها إلى آخرها...      0         0.6  \n",
       "1  لتعاليم بولسء ويعدونها من الهرطقة واندثرت هذه ...      1         0.8  \n",
       "2  أمي الحبيية ماذا لا قدين لي يدكدة كنت أعيث فقط...      0         0.8  \n",
       "3   أي نعم كيف لا أتذكر  صارت خمسة وعشرين ألف دول...      0         0.8  \n",
       "4  بدت الحدرة وهما يغادرانها أكثر حزبا وأكثر شيخو...      1         0.6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dir = os.getcwd()+\"/labeled_dataset\"\n",
    "print(dataset_dir)\n",
    "dataset = read_json_files(dataset_dir)\n",
    "\n",
    "display(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da64cf6b-f2f0-4884-9e97-695e4ef5d1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Statistics for the dataset:\n",
      "               book_name  page_number  \\\n",
      "count               4891  4891.000000   \n",
      "unique                24          NaN   \n",
      "top     خماسية مدن الملح          NaN   \n",
      "freq                1804          NaN   \n",
      "mean                 NaN   489.038847   \n",
      "std                  NaN   520.073097   \n",
      "min                  NaN     2.000000   \n",
      "25%                  NaN   115.000000   \n",
      "50%                  NaN   272.000000   \n",
      "75%                  NaN   639.500000   \n",
      "max                  NaN  1945.000000   \n",
      "\n",
      "                                               text_chunk        label  \\\n",
      "count                                                4891  4891.000000   \n",
      "unique                                               4891          NaN   \n",
      "top      يا سيدي الله يخليك المسألة من أولها إلى آخرها...          NaN   \n",
      "freq                                                    1          NaN   \n",
      "mean                                                  NaN     0.454508   \n",
      "std                                                   NaN     0.497977   \n",
      "min                                                   NaN     0.000000   \n",
      "25%                                                   NaN     0.000000   \n",
      "50%                                                   NaN     0.000000   \n",
      "75%                                                   NaN     1.000000   \n",
      "max                                                   NaN     1.000000   \n",
      "\n",
      "         confidence  \n",
      "count   4891.000000  \n",
      "unique          NaN  \n",
      "top             NaN  \n",
      "freq            NaN  \n",
      "mean       0.696994  \n",
      "std        0.126674  \n",
      "min        0.600000  \n",
      "25%        0.600000  \n",
      "50%        0.600000  \n",
      "75%        0.800000  \n",
      "max        1.000000  \n",
      "\n",
      "\n",
      "Statistics for the 'label' column:\n",
      "count    4891.000000\n",
      "mean        0.454508\n",
      "std         0.497977\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         1.000000\n",
      "max         1.000000\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print statistics for the entire DataFrame\n",
    "print(\"General Statistics for the dataset:\")\n",
    "print(dataset.describe(include='all'))  # 'include=all' to get statistics for all columns\n",
    "\n",
    "# Print a blank line for better readability\n",
    "print(\"\\n\")\n",
    "\n",
    "# Since our 'Label' column is categorical, we specify include='all' to get its statistics\n",
    "print(\"Statistics for the 'label' column:\")\n",
    "print(dataset['label'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "829228fa-c193-4ace-a308-8ee4f4ac317d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of 0s and 1s in the 'label' column:\n",
      "label\n",
      "0    2668\n",
      "1    2223\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of 0s and 1s in the 'Label' column\n",
    "label_counts = dataset['label'].value_counts()\n",
    "\n",
    "print(\"Counts of 0s and 1s in the 'label' column:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53107d72-deea-456a-b698-e1f28521bdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_name</th>\n",
       "      <th>page_number</th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>خماسية مدن الملح</td>\n",
       "      <td>1501</td>\n",
       "      <td>أشعث أغبر دميم الثياب عافي الشعر قلت أي الطعام...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>الحارس في حقل الشوفان</td>\n",
       "      <td>58</td>\n",
       "      <td>إلى أي مكان سوف تذهب أنت وفناتك هل حددت مكان ل...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>خماسية مدن الملح</td>\n",
       "      <td>215</td>\n",
       "      <td>وحين يسمع يستعيد يطرب يسافر بعيداء ويروي الكثي...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>خماسية مدن الملح</td>\n",
       "      <td>956</td>\n",
       "      <td>لدقائق بدا نمر عاجرا عن فهم الكلمات التي سمعها...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>خماسية مدن الملح</td>\n",
       "      <td>1186</td>\n",
       "      <td>ولم يتأخر السلطان ليدرك ما هو ممكن في هذه المر...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  book_name  page_number  \\\n",
       "1053       خماسية مدن الملح         1501   \n",
       "410   الحارس في حقل الشوفان           58   \n",
       "396        خماسية مدن الملح          215   \n",
       "1776       خماسية مدن الملح          956   \n",
       "614        خماسية مدن الملح         1186   \n",
       "\n",
       "                                             text_chunk  label  confidence  \n",
       "1053  أشعث أغبر دميم الثياب عافي الشعر قلت أي الطعام...      1         0.8  \n",
       "410   إلى أي مكان سوف تذهب أنت وفناتك هل حددت مكان ل...      1         0.8  \n",
       "396   وحين يسمع يستعيد يطرب يسافر بعيداء ويروي الكثي...      0         0.6  \n",
       "1776  لدقائق بدا نمر عاجرا عن فهم الكلمات التي سمعها...      0         0.6  \n",
       "614   ولم يتأخر السلطان ليدرك ما هو ممكن في هذه المر...      0         0.6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Dataset: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_name</th>\n",
       "      <th>page_number</th>\n",
       "      <th>text_chunk</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>نظرية السياسة الخارجية</td>\n",
       "      <td>162</td>\n",
       "      <td>يا ليامة لشارحية احتجزث كنذا ماني قوارب أمريك...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>حكاية الجارية</td>\n",
       "      <td>147</td>\n",
       "      <td>أوما براسه في رزانة تستحيل معرفة هل كان يعني م...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2977</th>\n",
       "      <td>نقد العقل العربي</td>\n",
       "      <td>357</td>\n",
       "      <td>ال فستيل في لاسي بفسر  جزنيا على الأقل  الظاهر...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3827</th>\n",
       "      <td>بنات حواء الثلاث</td>\n",
       "      <td>243</td>\n",
       "      <td>انم تفرست في السججادة المثبتة على الأرضية فلاح...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>نظرية السياسة الخارجية</td>\n",
       "      <td>48</td>\n",
       "      <td>ظرية ليمة الارحة ونسجل أيضا أثالم نطلق أحكاما...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   book_name  page_number  \\\n",
       "1943  نظرية السياسة الخارجية          162   \n",
       "1656           حكاية الجارية          147   \n",
       "2977        نقد العقل العربي          357   \n",
       "3827        بنات حواء الثلاث          243   \n",
       "2017  نظرية السياسة الخارجية           48   \n",
       "\n",
       "                                             text_chunk  label  confidence  \n",
       "1943   يا ليامة لشارحية احتجزث كنذا ماني قوارب أمريك...      1         0.8  \n",
       "1656  أوما براسه في رزانة تستحيل معرفة هل كان يعني م...      1         0.6  \n",
       "2977  ال فستيل في لاسي بفسر  جزنيا على الأقل  الظاهر...      0         0.6  \n",
       "3827  انم تفرست في السججادة المثبتة على الأرضية فلاح...      0         1.0  \n",
       "2017   ظرية ليمة الارحة ونسجل أيضا أثالم نطلق أحكاما...      1         0.8  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset, eval_dataset = train_test_split(dataset, test_size=0.2)\n",
    "print(\"Train Dataset: \")\n",
    "display(train_dataset.head())\n",
    "print(\"Eval Dataset: \")\n",
    "display(eval_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5268ac9a-eb9a-4641-9ff8-12511b130bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted Tokenization Function to Work on Lists\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize the Entire Training and Evaluation Datasets at Once\n",
    "tokenized_train = tokenize_function(train_dataset[\"text_chunk\"].tolist())\n",
    "tokenized_eval = tokenize_function(eval_dataset[\"text_chunk\"].tolist())\n",
    "\n",
    "# Prepare the Datasets\n",
    "train_dataset = Dataset(tokenized_train, train_dataset[\"label\"].tolist())\n",
    "eval_dataset = Dataset(tokenized_eval, eval_dataset[\"label\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a9f2f48-cba9-4df3-8d5a-d93cc8ea2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d148084b-26fc-47d2-83db-d86ed1f204f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yousef-tarek-st/notebookenv/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optimizer and Learning Rate Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 10\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e175c9-636c-490a-beb0-fc6c7b2904ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b5dfb7900b47e7b8aea65e366f9f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Epoch 1/10):   0%|          | 0/123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 finished. Training Loss: 0.7036830414601458. Training Accuracy: 0.522239263803681. Evaluation Loss: 0.6923362247405513. Evaluation Accuracy: 0.5280898876404494\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5109f0df69e14b20adf71dd83c606b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training (Epoch 2/10):   0%|          | 0/123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize lists to store per-epoch metrics for both loss and accuracy\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "train_accuracies = []\n",
    "eval_accuracies = []\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Training (Epoch {epoch+1}/{num_epochs})\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        correct_predictions += (preds == batch[\"labels\"]).sum().item()\n",
    "        total_predictions += preds.size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # Calculate average loss and accuracy over the epoch for training\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            total_eval_loss += outputs.loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            correct_predictions += (preds == batch[\"labels\"]).sum().item()\n",
    "            total_predictions += preds.size(0)\n",
    "    \n",
    "    # Calculate average evaluation loss and accuracy over the epoch\n",
    "    avg_eval_loss = total_eval_loss / len(eval_loader)\n",
    "    eval_accuracy = correct_predictions / total_predictions\n",
    "    eval_losses.append(avg_eval_loss)\n",
    "    eval_accuracies.append(eval_accuracy)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} finished. Training Loss: {avg_train_loss}. Training Accuracy: {train_accuracy}. Evaluation Loss: {avg_eval_loss}. Evaluation Accuracy: {eval_accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d587e-7566-48d8-9560-b4ff24f39d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and evaluation losses\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(eval_losses, label='Evaluation Loss')\n",
    "plt.title('Training and Evaluation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and evaluation accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(eval_accuracies, label='Evaluation Accuracy')\n",
    "plt.title('Training and Evaluation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(os.getcwd()+'/plots/training_evaluation_plots.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9907b12f-57b6-46c4-a3c2-c553a6f4d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = os.getcwd() + \"/AraBERT_fine_tuned_model\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer_save_path = os.getcwd() + \"/AraBERT_fine_tuned_tokenizer\"\n",
    "tokenizer.save_pretrained(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ab602a-eca0-4dca-90cc-7448fc491e12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
